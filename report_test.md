# Big Data and Commercial Data Analytics :hand: fa18-523-85 (finished the analysis, writing the paper)

| Bo Li
| bl15@iu.edu
| Indiana University Bloomington
| hid: fa18-523-85 
| github: [:cloud:](https://github.com/cloudmesh-community/fa18-523-85/blob/master/paper/paper.md)


## Abstract

As internet developed, online shopping has become part of our daily life. Black Friday, a traditional deal day, has also transformed as a big day for online shopping. The internet retailers, such Amazon, also developed their specific strategy for the combat in online shopping, Amazon Prime Day. In the e-commerce area, the volume of the sales and the product data increased rapidly. It is necessary to develop a cost-effective way to deal with the big data. The technique of relational data management has developed a lot in the last decades. In today's business analysis scenario, the relational technologies seem can not hold the large data since they are designed to deal with data which is much smaller in size. The developed internet techniques allow us collect and store the trading data, which could be the most valuable materials for researching customer behaviors. 


## Keywords

Big data, human behavior, data mining, Python   
 
## Introduction

### Commercial Data   

In order to extract the knowledge behind the commercial data generated by hundreds of thousands of consumers for the use of leading managers to make the decision, it is necessary to conduct a deep analysis to the commercial data, instead of generating simple reports. The deep analysis could hardly be done by SQL since the process relies on complex models. Without those models, it is impossible to get a profound understand of the commercial data [Comparing Business Intelligence and Big Data Skills]. People will not only need to find out what is happening now, but also need to use data to make some predictions in order to make preparations for the future events. For example, if the manager is able to predict the loss of the customer in the future, they can use discount to attract the users again.   
   
> “The quality of a product or service is an important determinant of consumer satisfaction, brand performance, and long-term brand success” [Mining Marketing Meaning from Online Chatter: Strategic Brand Analysis of Big Data Using Latent Dirichlet Allocation].

In the context of big commercial data, the traditional OLAP operations are not enough anymore to meet the requirements, we also need path analysis, time series analysis, graph analysis, what-if analysis and some complex statistical models. Time series analysis, a useful method in the commercial data analysis since we have got lots of the trading historical data [Big data and management]. The managers want to get some patterns in the data in order to fine some chances to improve the revenue. By the trend analysis, they can even predict some chances in advance. In the financial area, analysts are able to develop some software to conduct the time series analysis of the trading data, and find some profitable trading patterns. After further verification, they can use those profitable trading patterns to conduct the real trade and make profits.   

> “Managers and researchers usually obtain measures of perceived quality from customers through surveys or interviews, which are typically based on limited samples administered periodically” [Mining Marketing Meaning from Online Chatter: Strategic Brand Analysis of Big Data Using Latent Dirichlet Allocation].   

Large-scale graph and network analysis also plays a key role in the commercial data analysis. The virtual social network is actually a description of the links between the entities. In the network, every independent entity will be converted to a node in the total graph, and the relationship between nodes will converted to the link. By conducting social network analysis, we can find some useful knowledge such as a small community in the whole group. This information could be used to advertise some new product if the community meets the requirement of target group. We can also combine the individual behavior analysis and the group behavior analysis.   

> “The design decisions that determine what will be measured also stem from interpretation. For example, in the case of social media data, there is a ‘data cleaning’ process: making decisions about what attributes and variables will be counted, and which will be ignored. This process is inherently subjective” [CRITICAL QUESTIONS FOR BIG DATA].


### Behavioral Data

> “The study of consumer analytics lies at the junction of Big Data and consumer behavior. Data provide behavioral insights about consumers; marketers translate those insights into market advantage. Analytics generally refers to tools that help find hidden patterns in data” [Big Data consumer analytics and the transformation of marketing].


Behavioral big data (BBD) refers to very large and rich multidimensional data sets on human and social behaviors, actions, and interactions, which have become available to companies, governments, and researchers. A growing number of researchers in social science and management fields acquire and analyze BBD for the purpose of extracting knowledge and scientific discoveries [Research dilemmas with behavioral big data. Big Data, 5(2), 98]. Besides, the online retailers also want to figure out the profound meanings behind the consumer’s actions. So the behavioral big data comes across with research area and industry area, which results in different research methods. We use the methods in research area to analysis the dataset with specific models designed to the dataset from Kaggle.
   

### Necessity of Big Data

Why we use big data method to conduct the analytics? The answer lies on the character of the real dataset generated in the real trading system. The system records the deal with some specific fields, which could be some information of price, time, discount, product information, product status, etc. The user behavior could also be recorded such as the action of adding to list, the time between order to payment, etc. If a user has the habit of adding many products to the list but only buy a few of them, you may observe the data and draw the conclusion that the user is rational and could hardly be affected by the advertisement. But if you have huge amount of data, maybe it is real time data, the only way to picture those consumers is establishing models and define the features, then use the big data methods to research them. 

> “The opportunities associated with data and analysis in different organizations have helped generate significant interest in commercial data analysis, which is often referred to as the techniques, technologies, systems, practices, methodologies, and applications that analyze critical business data to help an enterprise better understand its business and market and make timely business decisions” [Business Intelligence and Analytics: From Big Data to Big Impact].

The user behavior could also be recorded such as the action of adding to list, the time between order to payment, etc. If a user has the habit of adding many products to the list but only buy a few of them, you may observe the data and draw the conclusion that the user is rational and could hardly be affected by the advertisement. But if you have huge amount of data, maybe it is real time data, the only way to picture those consumers is establishing models and define the features, then use the big data methods to research them. 


## Dataset

### Commercial Data   

The basic concept of commercial data is very easy to understand. The method of analyzing sales data provides a view to have a deep understanding of the sales data, which enables the managers to make a specific plan to conduct some strategies.   

> “Technology's changing pace requires faster market analyses than traditional market analytics can handle. BDA might provide the real-time speed necessary to meet this challenge” [Effects of big data analytics and traditional marketing analytics on new product success: A knowledge fusion perspective].

In the process of analysis of the commercial data, they often analyze the historical price, and the design of product line to mine the relationships behind different departments, which could be a still base for a better sale strategy. There are some specific requirements of the commercial data, such as it must be objective and reliable, or it may mislead the decisions of the managers.   

> “They make decisions based on rigorous analysis at more than double the rate of lower performers. The correlation between performance and analytics-driven management has important implications to organizations, whether they are seeking growth, efficiency or competitive differentiation” [Big Data, Analytics and the Path From Insights to Value].



### Dataset Description

The dataset we use here is from a retail store, it contains the information of transactions. To get a profound understand of customer behavior in different things. The first step is to conduct a descriptive analysis of the dataset to get an overview. The second step is to make a deeper study with different variables such as gender, age, city. If the owner wants to find the potential links between different product categorizes or specific links between group of people and products, they can do a cross analysis with different variables. The dataset is an ideal simple for classification and also clustering since all the needed information of the users is provided [https://www.kaggle.com/mehdidag/black-friday/home].   


Data columns (total 12 columns): User_ID: 537577 non-null int64, Product_ID: 537577 non-null object, Gender: 537577 non-null object, Age: 537577 non-null object, Occupation: 537577 non-null int64, City_Category: 537577 non-null object, Stay_In_Current_City_Years: 537577 non-null object, Marital_Status: 537577 non-null int64, Product_Category_1: 537577 non-null int64, Product_Category_2: 370591 non-null float64, Product_Category_3: 164278 non-null float64, Purchase: 537577 non-null int64   
  

## Tools

### Python 3.7   
Python is an easy to learn, powerful programming language. It has efficient high-level data structures and a simple but effective approach to object-oriented programming. Python’s elegant syntax and dynamic typing, together with its interpreted nature, make it an ideal language for scripting and rapid application development in many areas on most platforms [https://docs.python.org/3/tutorial/].   

### Numpy   
NumPy, which stands for Numerical Python, is a library consisting of multidimensional array objects and a collection of routines for processing those arrays. Using NumPy, mathematical and logical operations on arrays can be performed. This is a tutorial explains the basics of NumPy such as its architecture and environment. It also discusses the various array functions, types of indexing, etc. An introduction to Matplotlib is also provided. All this is explained with the help of examples for better understanding [https://www.tutorialspoint.com/numpy].   

### Pandas    
Pandas is a Python package providing fast, flexible, and expressive data structures designed to make working with “relational” or “labeled” data both easy and intuitive. It aims to be the fundamental high-level building block for doing practical, real world data analysis in Python. Additionally, it has the broader goal of becoming the most powerful and flexible open source data analysis, manipulation tool available in any language. It is already well on its way toward this goal. Pandas is well suited for many different kinds of data. Tabular data with heterogeneously-typed columns, as in an SQL table or Excel spreadsheet. Ordered and unordered (not necessarily fixed-frequency) time series data. Arbitrary matrix data (homogeneously typed or heterogeneous) with row and column labels. Any other form of observational / statistical data sets. The data actually need not be labeled at all to be placed into a pandas data structure [https://pandas.pydata.org/pandas-docs/stable/index.html].   

### Matplotlib.pyplot   
Matplotlib.pyplot is a collection of command style functions that make matplotlib work like MATLAB. Each pyplot function makes some change to a figure: e.g., creates a figure, creates a plotting area in a figure, plots some lines in a plotting area, decorates the plot with labels, etc. In matplotlib.pyplot various states are preserved across function calls, so that it keeps track of things like the current figure and plotting area, and the plotting functions are directed to the current axes (please note that “axes” here and in most places in the documentation refers to the axes part of a figure and not the strict mathematical term for more than one axis) [https://matplotlib.org/users/pyplot_tutorial.html].   

### Seaborn   
Seaborn is a Python visualization library for statistical plotting. It comes equipped with preset styles and color palettes so you can create complex, aesthetically pleasing charts with a few lines of code. It’s designed to work with NumPy and pandas data structures and to support statistical tasks completed in SciPy and statsmodels. Seaborn is built on top of Python’s core visualization library matplotlib, but it’s meant to serve as a complement, not a replacement. In most cases, you’ll still use matplotlib for simple plotting, and you’ll need a knowledge of matplotlib to tweak Seaborn’s default plots [https://community.modeanalytics.com/python/libraries/seaborn/].   



## Implementation   
### Data Cleaning   

Data cleaning is a necessary step for the whole process of data cleaning. The result of the data cleaning will have significant impact on the efficiency of the model and the conclusions. In the practice, data cleaning often takes some time. There is also a domain about how to clean the dataset effectively. When multiple data sources need to be integrated, e.g., in data warehouses, federated database systems or global web-based information systems, the need for data cleaning increases significantly [Data Cleaning: Problems and Current Approaches].

#### Pre-process   
There are two things need to be done in this step. It is recommended to use the database to store the data since there are lots of advantages of this. If the size of the dataset is too huge to operate in the database, we can also store the data in text and operate it in the Python. The second thing is to have a overview of the data. Checking with the original data is a good way, but the dataset is often too big to get a overview. So extract a simple of the dataset is another choice of this, which will allow you to have better understanding of the original dataset.
#### Null test   
Missing value is a common problem of the data analysis, and there are a lot of ways to deal with this problem. We often do this job in four steps.   
Locate the range of the missing values. In order to distinguish different variables with different importance, we calculate the missing percentage of each part of the dataset. According to the result of the calculation, we use different strategies to deal with those missing values.   

# fig1   
# fig2   

To the part with high importance and low missing rate, we give some values to the missing part by calculating. In some cases, we also use our experience to make up the missing part. To the part with high importance and high missing rate, we will try to fix the problem by finding other data sources. In some troublesome cases, we even delete the whole part of the dataset and claim the action in the result. To the part with low importance and low missing rate, we can care other things or estimate the missing value by some simple calculations. To the part of low importance and high missing rate, we choose to delete the whole part since it can not have a significant impact on the result of the analysis.   

# fig3   







### Data Exploration and Processing   

### Data Analysis and Data Visualization   



## Benchmark

## Conclusion

The conclusion drawn by the previous analytics, which could be the picture of owned consumers, the predict of the consumer’s preferences.





